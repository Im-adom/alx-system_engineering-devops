It was a tough period for me from 10:00 AM to 15:15 PM (GMT) on the 3rd of October, 2023.
The situation made me a bit  frustrated because my ability to access the server was completely disrupted. 

It all came down to an unexpected server misconfiguration. Fortunately, I managed to fix the issue by identifying and rectifying the specific misconfiguration.

The stress began at 11:30 AM (GMT) when I found myself unable to establish a secure connection to the server.
I noticed that something wasn't right with the server access.

My investigation involved diving into server logs, checking network connectivity, and reviewing recent server configuration updates. I initially speculated network failures or a potential security breach, but they didn't lead to a solution. I eventually found the issue within the server's access control list settings.

At some points, I focused on recent server changes and briefly ended up down the wrong track. There was also a brief concern about a potential DDoS attack.

 I reached out to our system administrators and network engineers responsible for server maintenance and configuration.

Finally i pinpointed and resolved the misconfiguration, restoring normal server access.

Corrective and Preventative Measures:
Fixes: I have amped audit processes for server configuration changes and now have better change management protocols in place. I've also received additional training on server configuration verification.

This is basically a postmortem represents my personal experience with the outage, recounting the duration, impact, root cause, detection, actions taken, misleading paths, escalation, resolution, and our plans for corrective/preventative measures. Itâ€™s a journey of challenges and solutions tailored to my experiences.
But well...
Who doesn't make mistakes ðŸ¤­
